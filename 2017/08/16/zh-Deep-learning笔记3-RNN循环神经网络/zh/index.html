<!DOCTYPE html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Deep learning笔记3-RNN循环神经网络 | H.J.T. Github 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="H.J.T.">
    
    

    <meta name="description" content="1. 循环神经网络（RNN）原图和公式说明来自：零基础入门深度学习(5) - 循环神经网络 

 


x是一个向量，表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；
s是一个向量，表示隐藏层的值（这里隐藏层面画了一个节点，也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；
U是输入层到隐藏层的权重矩阵；
o是一个向量，表示输出层的值；
V是隐藏层到输出层的权重矩阵。
W权重矩阵">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep learning笔记3-RNN循环神经网络 | H.J.T. Github">
<meta property="og:url" content="https://hjtso.github.io/2017/08/16/zh-Deep-learning笔记3-RNN循环神经网络/zh/index.html">
<meta property="og:site_name" content="H.J.T. Github">
<meta property="og:description" content="1. 循环神经网络（RNN）原图和公式说明来自：零基础入门深度学习(5) - 循环神经网络 

 


x是一个向量，表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；
s是一个向量，表示隐藏层的值（这里隐藏层面画了一个节点，也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；
U是输入层到隐藏层的权重矩阵；
o是一个向量，表示输出层的值；
V是隐藏层到输出层的权重矩阵。
W权重矩阵">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/1-1.jpg">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/1-2.jpg">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/1-3.jpg">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/2-1.jpg">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/2-2.jpg">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/2-3.jpg">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/3-1.jpg">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/3-2.jpg">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/4-1.png">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/4-2.png">
<meta property="og:image" content="https://hjtso.github.io/image/DL/3/4-3.jpg">
<meta property="og:updated_time" content="2021-01-11T06:41:34.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep learning笔记3-RNN循环神经网络 | H.J.T. Github">
<meta name="twitter:description" content="1. 循环神经网络（RNN）原图和公式说明来自：零基础入门深度学习(5) - 循环神经网络 

 


x是一个向量，表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；
s是一个向量，表示隐藏层的值（这里隐藏层面画了一个节点，也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；
U是输入层到隐藏层的权重矩阵；
o是一个向量，表示输出层的值；
V是隐藏层到输出层的权重矩阵。
W权重矩阵">
<meta name="twitter:image" content="https://hjtso.github.io/image/DL/3/1-1.jpg">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon-black-55.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">H.J.T. Github</a></h1>
        <hr class="panel-cover__divider" />

        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">See</a></li>
              
                
                <li class="navigation__item"><a href="/About" title="" class="">Key</a></li>
              
                
                <li class="navigation__item"><a href="/Archive" title="" class="">Arc</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">

    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/hjtso" title="Huno on GitHub">
          <i class='icon icon-social-github'></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

      <!-- Facebook -->
      <li class="navigation__item">
        <a href="https://www.facebook.com/jintao.huang.1" title=“Facebook”>
          <i class='icon icon-social-facebook'></i>
          <span class="label">Facebook</span>
        </a>
      </li>

      <!-- Instagram -->
      <li class="navigation__item">
        <a href="https://www.instagram.com/huang_jintao/" title=“Instagram”>
          <i class='icon icon-social-instagram'></i>
          <span class="label">Instagram</span>
        </a>
      </li>

      <!-- Linkedin -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/jintao-huang-48429565" title=“Linkedin”>
          <i class='icon icon-social-linkedin'></i>
          <span class="label">Linkedin</span>
        </a>
      </li>
	  
    <!-- China social icon -->

      <li class="navigation__item">
        <a href="http://www.weibo.com/musimusi" title=“Weibo”>
          <i class='icon cs-icon-weibo'></i>
          <span class="label">Weibo</span>
        </a>
      </li>

    <!--
      <li class="navigation__item">
        <a href="https://www.hjt.so" title=“_hjtso”>
          <i class='icon cs-icon-wechat'></i>
          <span class="label">Wechat</span>
        </a>
      </li>


      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li>
    -->


  </ul>
</nav>



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Deep learning笔记3-RNN循环神经网络</h1>

    

    <div class="post-meta">
      <time datetime="2017-08-16" class="post-meta__date date">2017-08-16</time> 

      <span class="post-meta__tags tags">

          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/Deep-Learning/">Deep Learning</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <h3 id="1-循环神经网络（RNN）"><a href="#1-循环神经网络（RNN）" class="headerlink" title="1. 循环神经网络（RNN）"></a>1. 循环神经网络（RNN）</h3><p>原图和公式说明来自：<a href="https://zybuluo.com/hanbingtao/note/541458" title="Title" target="_blank" rel="external">零基础入门深度学习(5) - 循环神经网络</a> </p>
<hr>
<center><img src="/image/DL/3/1-1.jpg" alt="RNN"></center> 

<hr>
<p>x是一个向量，表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；</p>
<p>s是一个向量，表示隐藏层的值（这里隐藏层面画了一个节点，也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；</p>
<p>U是输入层到隐藏层的权重矩阵；</p>
<p>o是一个向量，表示输出层的值；</p>
<p>V是隐藏层到输出层的权重矩阵。</p>
<p>W权重矩阵是隐藏层上一次的值作为这一次的输入的权重矩阵。</p>
<h4 id="1-1-循环神经网络的计算"><a href="#1-1-循环神经网络的计算" class="headerlink" title="1.1. 循环神经网络的计算"></a>1.1. 循环神经网络的计算</h4><h5 id="1-1-1-基本循环神经网络"><a href="#1-1-1-基本循环神经网络" class="headerlink" title="1.1.1. 基本循环神经网络"></a>1.1.1. 基本循环神经网络</h5><p>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t)\qquad\qquad\quad(输出层)\\<br>\mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})\qquad(隐藏层)\\<br>\end{align}</p>
<p>如果反复把隐藏层带入到输出层得到：</p>
<p>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t)\\<br>&amp;=Vf(U\mathrm{x}_t+W\mathrm{s}_{t-1})\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+W\mathrm{s}_{t-2}))\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+W\mathrm{s}_{t-3})))\\<br>&amp;=Vf(U\mathrm{x}_t+Wf(U\mathrm{x}_{t-1}+Wf(U\mathrm{x}_{t-2}+Wf(U\mathrm{x}_{t-3}+…))))<br>\end{align}</p>
<h5 id="1-1-2-双向循环神经网络"><a href="#1-1-2-双向循环神经网络" class="headerlink" title="1.1.2. 双向循环神经网络"></a>1.1.2. 双向循环神经网络</h5><hr>
<center><img src="/image/DL/3/1-2.jpg" alt="RNN"></center> 

<hr>
<p>最终的输出值取决于和。其计算方法为：</p>
<p>\begin{align}<br>\mathrm{y}_2=g(VA_2+V’A_2’)<br>\end{align}</p>
<p>其中:</p>
<p>\begin{align}<br>A_2&amp;=f(WA_1+U\mathrm{x}_2)\\<br>A_2’&amp;=f(W’A_3’+U’\mathrm{x}_2)\\<br>\end{align}</p>
<p>正向计算时，隐藏层的值St与St-1有关；反向计算时，隐藏层的值S’t与S’t+1有关；最终的输出取决于正向和反向计算的加和。双向循环神经网络的计算方法：</p>
<p>\begin{align}<br>\mathrm{o}_t&amp;=g(V\mathrm{s}_t+V’\mathrm{s}_t’)\\<br>\mathrm{s}_t&amp;=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})\\<br>\mathrm{s}_t’&amp;=f(U’\mathrm{x}_t+W’\mathrm{s}_{t+1}’)\\<br>\end{align}</p>
<p>正向计算和反向计算不共享权重: U和U’、W和W’、V和V’都是不同的权重矩阵。</p>
<h5 id="1-1-3-深度循环神经网络"><a href="#1-1-3-深度循环神经网络" class="headerlink" title="1.1.3. 深度循环神经网络"></a>1.1.3. 深度循环神经网络</h5><hr>
<center><img src="/image/DL/3/1-3.jpg" alt="RNN"></center> 

<hr>
<p>堆叠两个以上的隐藏层，就得到了深度循环神经网络。其计算方法为：</p>
<p>\begin{align}<br>\mathrm{o}_t&amp;=g(V^{(i)}\mathrm{s}_t^{(i)}+V’^{(i)}\mathrm{s}_t’^{(i)})\\<br>\mathrm{s}_t^{(i)}&amp;=f(U^{(i)}\mathrm{s}_t^{(i-1)}+W^{(i)}\mathrm{s}_{t-1})\\<br>\mathrm{s}_t’^{(i)}&amp;=f(U’^{(i)}\mathrm{s}_t’^{(i-1)}+W’^{(i)}\mathrm{s}_{t+1}’)\\<br>…\\<br>\mathrm{s}_t^{(1)}&amp;=f(U^{(1)}\mathrm{x}_t+W^{(1)}\mathrm{s}_{t-1})\\<br>\mathrm{s}_t’^{(1)}&amp;=f(U’^{(1)}\mathrm{x}_t+W’^{(1)}\mathrm{s}_{t+1}’)\\<br>\end{align}</p>
<h4 id="1-2-循环神经网络的训练"><a href="#1-2-循环神经网络的训练" class="headerlink" title="1.2. 循环神经网络的训练"></a>1.2. 循环神经网络的训练</h4><ul>
<li>循环神经网络的训练算法：BPTT  </li>
</ul>
<p>先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w。</p>
<ul>
<li>前向计算</li>
</ul>
<p>\begin{align}<br>\mathrm{s}_t=f(U\mathrm{x}_t+W\mathrm{s}_{t-1})<br>\end{align}</p>
<ul>
<li>误差项的计算</li>
</ul>
<p>BTPP算法将第l层t时刻的误差项值沿两个方向传播：</p>
<p>① 是沿空间传递到上一层网络，这部分只和权重矩阵U有关：</p>
<p>\begin{align}<br>(\delta_t^{l-1})^T=&amp;(\delta_t^l)^TUdiag[f’^{l-1}(\mathrm{net}_t^{l-1})]<br>\end{align}</p>
<p>② 是沿时间线传递到初始时刻，这部分只和权重矩阵W有关：</p>
<p>\begin{align}<br>\delta_k^T=&amp;\delta_t^T\prod_{i=k}^{t-1}Wdiag[f’(\mathrm{net}_{i})]<br>\end{align}</p>
<p>RNN在训练中很容易发生梯度爆炸和梯度消失（取决于大于1还是小于1），这导致训练时梯度不能在较长序列中一直传递下去，从而使RNN无法捕捉到长距离的影响。</p>
<h3 id="2-长短时记忆网络（LSTM）"><a href="#2-长短时记忆网络（LSTM）" class="headerlink" title="2. 长短时记忆网络（LSTM）"></a>2. 长短时记忆网络（LSTM）</h3><p>原图和公式说明来自：- <a href="https://zybuluo.com/hanbingtao/note/581764" title="Title" target="_blank" rel="external">零基础入门深度学习(6) - 长短时记忆网络</a> </p>
<hr>
<center><img src="/image/DL/3/2-1.jpg" alt="RNN"></center> 

<hr>
<p>长短时记忆网络相对于普通循环神经网络，增加了一个状态c来保存长期的状态。</p>
<hr>
<center><img src="/image/DL/3/2-2.jpg" alt="RNN"></center> 

<hr>
<h4 id="2-1-长短时记忆网络的计算"><a href="#2-1-长短时记忆网络的计算" class="headerlink" title="2.1. 长短时记忆网络的计算"></a>2.1. 长短时记忆网络的计算</h4><hr>
<center><img src="/image/DL/3/2-3.jpg" alt="RNN"></center> 

<hr>
<p>\begin{align}<br>g(\mathbf{x})=\sigma(W\mathbf{x}+\mathbf{b})<br>\end{align}</p>
<p>● 遗忘门（forget gate）决定了上一时刻的单元状态有多少保留到当前时刻：</p>
<p>\begin{align}<br>\mathbf{f}_t=\sigma(W_f\cdot[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_f)<br>\end{align}</p>
<p>● 输入门（input gate）决定了当前时刻网络的输入有多少保存到单元状态：</p>
<p>\begin{align}<br>\mathbf{i}_t=\sigma(W_i\cdot[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_i)<br>\end{align}</p>
<p>● 输出门（output gate）来控制单元状态有多少输出到LSTM的当前输出值：</p>
<p>\begin{align}<br>\mathbf{\tilde{c}}_t=\tanh(W_c\cdot[\mathbf{h}_{t-1},\mathbf{x}_t]+\mathbf{b}_c)<br>\end{align}</p>
<p>推荐阅读 - <a href="http://blog.qiangzhonghua.com/blog/tech/lstm#15-%E9%81%97%E7%95%99%E9%97%AE%E9%A2%98---%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%85%AC%E5%BC%8F" title="Title" target="_blank" rel="external">强的笔记-我对LSTM的理解</a></p>
<hr>
<h4 id="2-2-长短时记忆网络的训练"><a href="#2-2-长短时记忆网络的训练" class="headerlink" title="2.2. 长短时记忆网络的训练"></a>2.2. 长短时记忆网络的训练</h4><p>先前向传播，再反向传播，利用链式求导计算损失函数对每个权重的偏导数（梯度），然后再根据梯度下降公式更新权重w。</p>
<p>第l层t时刻的误差项值沿两个方向传播：</p>
<p>① 是沿空间传递到上一层网络，这部分只和权重矩阵U有关：</p>
<p>\begin{align}<br>\frac{\partial{E}}{\partial{\mathbf{net}_t^{l-1}}}&amp;=(\delta_{f,t}^TW_{fx}+\delta_{i,t}^TW_{ix}+\delta_{\tilde{c},t}^TW_{cx}+\delta_{o,t}^TW_{ox})\circ f’(\mathbf{net}_t^{l-1})<br>\end{align}</p>
<p>② 是沿时间线传递到初始时刻，这部分只和权重矩阵W有关：</p>
<p>\begin{align}<br>\delta_k^T=\prod_{j=k}^{t-1}\delta_{o,j}^TW_{oh}<br>+\delta_{f,j}^TW_{fh}<br>+\delta_{i,j}^TW_{ih}<br>+\delta_{\tilde{c},j}^TW_{ch}<br>\end{align}</p>
<h3 id="3-基于TensorFlow的实现（RNN-LSTM-with-TF）"><a href="#3-基于TensorFlow的实现（RNN-LSTM-with-TF）" class="headerlink" title="3. 基于TensorFlow的实现（RNN/LSTM with TF）"></a>3. 基于TensorFlow的实现（RNN/LSTM with TF）</h3><p>TensorFlow中的RNN的抽象基类“ <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/RNNCell" title="Title" target="_blank" rel="external">RNNCell</a>”，是实现RNN的基本单元。</p>
<p>Github - <a href="https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/rnn_cell_impl.py" title="Title" target="_blank" rel="external">源码阅读</a></p>
<p>● call方法</p>
<p>每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)。</p>
<p>每调用一次RNNCell的call方法，就相当于在时间上推进了一步。</p>
<p>假设有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)：</p>
<hr>
<center><img src="/image/DL/3/3-1.jpg" alt="RNN"></center> 

<hr>
<p>再调用一次call(x2, h1)就可以得到(output2, h2)：</p>
<hr>
<center><img src="/image/DL/3/3-2.jpg" alt="RNN"></center> 

<hr>
<p>● 两个重要类变量</p>
<p>隐藏层的大小：state_size</p>
<p>输出层的大小：output_size</p>
<p>● 常用的三个子类</p>
<p><a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicRNNCell" title="Title" target="_blank" rel="external">BasicRNNCell</a></p>
<p><a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/BasicLSTMCell" title="Title" target="_blank" rel="external">BasicLSTMCell</a></p>
<p><a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/rnn/MultiRNNCell" title="Title" target="_blank" rel="external">MultiRNNCell</a></p>
<h4 id="3-1-单层LSTM例"><a href="#3-1-单层LSTM例" class="headerlink" title="3.1. 单层LSTM例"></a>3.1. 单层LSTM例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line">lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=<span class="number">128</span>)</div><div class="line">inputs = tf.placeholder(np.float32, shape=(<span class="number">32</span>, <span class="number">100</span>)) <span class="comment"># 32 是 batch_size</span></div><div class="line">h0 = lstm_cell.zero_state(<span class="number">32</span>, np.float32) <span class="comment"># 得到一个全0的初始状态</span></div><div class="line">output, h1 = lstm_cell.call(inputs, h0)</div><div class="line"></div><div class="line"><span class="comment"># LSTM可以看做有两个隐状态h和c，对应的隐层就是一个Tuple元组</span></div><div class="line"><span class="comment"># 每个都是(batch_size, state_size)的形状</span></div><div class="line">print(h1.h)  <span class="comment"># shape=(32, 128)</span></div><div class="line">print(h1.c)  <span class="comment"># shape=(32, 128)</span></div></pre></td></tr></table></figure>
<h4 id="3-2-一次执行多步"><a href="#3-2-一次执行多步" class="headerlink" title="3.2. 一次执行多步"></a>3.2. 一次执行多步</h4><p>TensorFlow提供了一个tf.nn.dynamic_rnn函数，使用该函数就相当于调用了n次call函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># inputs: shape = (batch_size, time_steps, input_size) </span></div><div class="line"><span class="comment"># cell: RNNCell</span></div><div class="line"><span class="comment"># initial_state: shape = (batch_size, cell.state_size)。初始状态。一般可以取0矩阵</span></div><div class="line">outputs, state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)</div></pre></td></tr></table></figure>
<p>官方 <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn" title="Title" target="_blank" rel="external">tf.nn.dynamic_rnn文档</a></p>
<h4 id="3-3-堆叠RNNCell"><a href="#3-3-堆叠RNNCell" class="headerlink" title="3.3. 堆叠RNNCell"></a>3.3. 堆叠RNNCell</h4><p>即实现多层的RNN。将x输入第一层RNN的后得到隐层状态h，这个隐层状态就相当于第二层RNN的输入，第二层RNN的隐层状态又相当于第三层RNN的输入，以此类推。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 每调用一次这个函数就返回一个BasicRNNCell</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_a_cell</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.nn.rnn_cell.BasicRNNCell(num_units=<span class="number">128</span>)</div><div class="line"><span class="comment"># 用tf.nn.rnn_cell MultiRNNCell创建3层RNN</span></div><div class="line">cell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>)]) <span class="comment"># 3层RNN</span></div><div class="line"><span class="comment"># 得到的cell实际也是RNNCell的子类</span></div><div class="line"><span class="comment"># 它的state_size是(128, 128, 128)</span></div><div class="line"><span class="comment"># (128, 128, 128)并不是128x128x128的意思</span></div><div class="line"><span class="comment"># 而是表示共有3个隐层状态，每个隐层状态的大小为128</span></div><div class="line">print(cell.state_size) <span class="comment"># (128, 128, 128)</span></div><div class="line"></div><div class="line"><span class="comment"># 使用对应的call函数</span></div><div class="line">inputs = tf.placeholder(np.float32, shape=(<span class="number">32</span>, <span class="number">100</span>)) <span class="comment"># 32 是 batch_size</span></div><div class="line">h0 = cell.zero_state(<span class="number">32</span>, np.float32) <span class="comment"># 通过zero_state得到一个全0的初始状态</span></div><div class="line">output, h1 = cell.call(inputs, h0)</div><div class="line"></div><div class="line">print(h1) <span class="comment"># tuple中含有3个32x128的向量</span></div></pre></td></tr></table></figure>
<p>MultiRNNCell也是RNNCell的子类，因此也有call方法、state_size和output_size变量。同样可以通过tf.nn.dynamic_rnn来一次运行多步。</p>
<ul>
<li>Attention 版本问题</li>
</ul>
<p>目前使用TensorFlow 1.2运行堆叠RNN的代码是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 每调用一次这个函数就返回一个BasicRNNCell</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_a_cell</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.nn.rnn_cell.BasicRNNCell(num_units=<span class="number">128</span>)</div><div class="line"><span class="comment"># 用tf.nn.rnn_cell MultiRNNCell创建3层RNN</span></div><div class="line">cell = tf.nn.rnn_cell.MultiRNNCell([get_a_cell() <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>)]) <span class="comment"># 3层RNN</span></div></pre></td></tr></table></figure>
<p>更早一些的版本中（比如TensorFlow 1.0.0），实现方式是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">one_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=<span class="number">128</span>)</div><div class="line">cell = tf.nn.rnn_cell.MultiRNNCell([one_cell] * <span class="number">3</span>) <span class="comment"># 3层RNN</span></div></pre></td></tr></table></figure>
<p>新版本按照这种的方式定义，就会引起报错。</p>
<p>阅读参考 - <a href="http://www.jianshu.com/p/b3c7883e3ddf" title="Title" target="_blank" rel="external">RNN入门：多层LSTM网络（四）</a> </p>
<h4 id="3-4-TF循环神经网络的实现例"><a href="#3-4-TF循环神经网络的实现例" class="headerlink" title="3.4. TF循环神经网络的实现例"></a>3.4. TF循环神经网络的实现例</h4><p>Make TV scripts using RNN(Recurrent Neural Networks) and LSTM(Long Short-Term Memory) network. Generate a new TV script with the model from TV scripts training data sets.</p>
<p>RNN(Recurrent Neural Networks)及びLSTM(Long Short-Term Memory)を使用して、訓練データを用いたモデルを作成して、新しいテレビスクリプトを生成する。</p>
<p>程序实例 - <a href="https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb" title="Title" target="_blank" rel="external">Github Link</a> </p>
<h3 id="4-词分析框架（Word-Framework）"><a href="#4-词分析框架（Word-Framework）" class="headerlink" title="4. 词分析框架（Word Framework）"></a>4. 词分析框架（Word Framework）</h3><h4 id="4-1-word2vec"><a href="#4-1-word2vec" class="headerlink" title="4.1. word2vec"></a>4.1. word2vec</h4><p>单词嵌入模型（简称为  word2vec 模型）。将word映射成连续（高维）向量，这样通过训练，就可以把对文本内容的处理简化为K维向量空间中向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。</p>
<p>一般来说， word2vec输出的词向量可以被用来做NLP相关的工作，比如聚类、找同义词、词性分析等等。</p>
<p>有两种实现的架构，CBOW(Continuous Bag-Of-Words)模型以及Skip-gram模型：</p>
<hr>
<center><img src="/image/DL/3/4-1.png" alt="RNN"></center> 

<hr>
<p>阅读链接 - <a href="http://www.cnblogs.com/tina-smile/p/5176441.html" title="Title" target="_blank" rel="external">word2vec入门基础</a></p>
<h4 id="4-2-seq2seq"><a href="#4-2-seq2seq" class="headerlink" title="4.2. seq2seq"></a>4.2. seq2seq</h4><p>序列到序列模型（简称为 seq2seq 模型）。seq2seq模型就像一个翻译模型,输入是一个序列(比如一个英文句子),输出也是一个序列(比如该英文句子所对应的法文翻译)。</p>
<hr>
<center><img src="/image/DL/3/4-2.png" alt="RNN"></center> 

<hr>
<p>● 编码器：这是一个<a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn" title="Title" target="_blank" rel="external">tf.nn.dynamic_rnn</a>函数。</p>
<p>● 解码器：这是一个<a href="https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/seq2seq/dynamic_rnn_decoder" title="Title" target="_blank" rel="external">tf.contrib.seq2seq.dynamic_rnn_decoder</a>函数。</p>
<hr>
<center><img src="/image/DL/3/4-3.jpg" alt="RNN"></center> 

<hr>
<p>阅读链接 - <a href="http://www.jianshu.com/p/124b777e0c55" title="Title" target="_blank" rel="external">Seq2Seq的DIY简介</a></p>
<p>阅读链接 - <a href="http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/" title="Title" target="_blank" rel="external">聊天机器人深度学习</a></p>
<p>阅读链接 - <a href="https://github.com/ematvey/tensorflow-seq2seq-tutorials" title="Title" target="_blank" rel="external">tensorflow-seq2seq-tutorials </a></p>
<hr>
<p>● word2vec输出的是一个词向量，seq2seq输出的是一个词序列。</p>
<p>推荐阅读 - <a href="http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/" title="Title" target="_blank" rel="external">Deep Learning for Chatbots, Part 1</a></p>
<p>推荐阅读 - <a href="https://zhuanlan.zhihu.com/p/27087310" title="Title" target="_blank" rel="external">《安娜卡列尼娜》文本生成</a></p>
<p>推荐阅读 - <a href="https://qiita.com/elm200/items/6f84d3a42eebe6c47caa" title="Title" target="_blank" rel="external">LSTMで夏目漱石ぽい文章の生成</a></p>
<h3 id="程序实例（Program-Example）"><a href="#程序实例（Program-Example）" class="headerlink" title="程序实例（Program Example）"></a>程序实例（Program Example）</h3><ul>
<li><p><a href="https://github.com/HJTSO/tv-script-generation/blob/master/dlnd_tv_script_generation.ipynb" title="Title" target="_blank" rel="external">Github Link - tv-script-generation</a> </p>
</li>
<li><p><a href="https://github.com/HJTSO/language-translation/blob/master/dlnd_language_translation.ipynb" title="Title" target="_blank" rel="external">Github Link - language-translation</a> </p>
</li>
</ul>
<h3 id="参考资料（Reference）"><a href="#参考资料（Reference）" class="headerlink" title="参考资料（Reference）"></a>参考资料（Reference）</h3><ul>
<li><p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" title="Title" target="_blank" rel="external">Understanding LSTM Networks</a> </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/28196873" title="Title" target="_blank" rel="external">TensorFlow中RNN实现</a> </p>
</li>
<li><p><a href="https://zybuluo.com/hanbingtao/note/541458" title="Title" target="_blank" rel="external">零基础入门深度学习(5) - 循环神经网络</a> </p>
</li>
<li><p><a href="https://zybuluo.com/hanbingtao/note/581764" title="Title" target="_blank" rel="external">零基础入门深度学习(6) - 长短时记忆网络</a> </p>
</li>
<li><p><a href="https://qiita.com/KojiOhki/items/89cd7b69a8a6239d67ca" title="Title" target="_blank" rel="external">LSTMネットワークの概要</a> </p>
</li>
<li><p><a href="https://qiita.com/t_Signull/items/21b82be280b46f467d1b" title="Title" target="_blank" rel="external">わかるLSTM</a> </p>
</li>
</ul>

  </section>

  
  
</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; H.J.T. | <a href="https://hexo.io/">Hexo</a> | Theme: <a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    

    <script src="/js/jquery.githubRepoWidget.min.js"></script>


    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
