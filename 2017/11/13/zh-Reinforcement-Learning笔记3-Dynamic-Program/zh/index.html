<!DOCTYPE html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Reinforcement Learning笔记3-Dynamic Program | H.J.T. Github 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="H.J.T.">
    
    

    <meta name="description" content="Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）动态规划是一种通过把复杂问题划分为子问题，并对自问题进行求解，最后把子问题的解结合起来解决原问题的方法。
「动态」是指问题由一系列的状态组成，而且状态能一步步地改变。
「规划」即优化每一个子问题。因为MDP 的 Markov 特性，即某一时刻的子问题仅仅取决于上一时刻的子问题的 action，并且 B">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning笔记3-Dynamic Program | H.J.T. Github">
<meta property="og:url" content="http://yoursite.com/2017/11/13/zh-Reinforcement-Learning笔记3-Dynamic-Program/zh/index.html">
<meta property="og:site_name" content="H.J.T. Github">
<meta property="og:description" content="Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）动态规划是一种通过把复杂问题划分为子问题，并对自问题进行求解，最后把子问题的解结合起来解决原问题的方法。
「动态」是指问题由一系列的状态组成，而且状态能一步步地改变。
「规划」即优化每一个子问题。因为MDP 的 Markov 特性，即某一时刻的子问题仅仅取决于上一时刻的子问题的 action，并且 B">
<meta property="og:image" content="http://yoursite.com/image/RL/3/1.jpg">
<meta property="og:image" content="http://yoursite.com/image/RL/3/2.jpg">
<meta property="og:image" content="http://yoursite.com/image/RL/3/3.jpg">
<meta property="og:image" content="http://yoursite.com/image/RL/3/4.png">
<meta property="og:updated_time" content="2017-10-25T13:33:21.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning笔记3-Dynamic Program | H.J.T. Github">
<meta name="twitter:description" content="Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）动态规划是一种通过把复杂问题划分为子问题，并对自问题进行求解，最后把子问题的解结合起来解决原问题的方法。
「动态」是指问题由一系列的状态组成，而且状态能一步步地改变。
「规划」即优化每一个子问题。因为MDP 的 Markov 特性，即某一时刻的子问题仅仅取决于上一时刻的子问题的 action，并且 B">
<meta name="twitter:image" content="http://yoursite.com/image/RL/3/1.jpg">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon-black-55.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">H.J.T. Github</a></h1>
        <hr class="panel-cover__divider" />

        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">See</a></li>
              
                
                <li class="navigation__item"><a href="/About" title="" class="">Key</a></li>
              
                
                <li class="navigation__item"><a href="/Archive" title="" class="">Arc</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">

    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/hjtso" title="Huno on GitHub">
          <i class='icon icon-social-github'></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

      <!-- Facebook -->
      <li class="navigation__item">
        <a href="https://www.facebook.com/jintao.huang.1" title=“Facebook”>
          <i class='icon icon-social-facebook'></i>
          <span class="label">Facebook</span>
        </a>
      </li>

      <!-- Instagram -->
      <li class="navigation__item">
        <a href="https://www.instagram.com/huang_jintao/" title=“Instagram”>
          <i class='icon icon-social-instagram'></i>
          <span class="label">Instagram</span>
        </a>
      </li>

      <!-- Linkedin -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/jintao-huang-48429565" title=“Linkedin”>
          <i class='icon icon-social-linkedin'></i>
          <span class="label">Linkedin</span>
        </a>
      </li>
    

    <!-- China social icon -->

      <li class="navigation__item">
        <a href="http://www.weibo.com/musimusi" title=“Weibo”>
          <i class='icon cs-icon-weibo'></i>
          <span class="label">Weibo</span>
        </a>
      </li>

    <!--
      <li class="navigation__item">
        <a href="https://www.hjt.so" title=“HJT347341”>
          <i class='icon cs-icon-wechat'></i>
          <span class="label">Wechat</span>
        </a>
      </li>


      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li>
    -->


  </ul>
</nav>



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Reinforcement Learning笔记3-Dynamic Program</h1>

    

    <div class="post-meta">
      <time datetime="2017-11-13" class="post-meta__date date">2017-11-13</time> 

      <span class="post-meta__tags tags">

          

          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <h4 id="Bellman最优解策略-动态规划法（Dynamic-Programming-Methods）"><a href="#Bellman最优解策略-动态规划法（Dynamic-Programming-Methods）" class="headerlink" title="Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）"></a>Bellman最优解策略 - 动态规划法（Dynamic Programming Methods）</h4><p>动态规划是一种通过把复杂问题划分为子问题，并对自问题进行求解，最后把子问题的解结合起来解决原问题的方法。</p>
<p>「动态」是指问题由一系列的状态组成，而且状态能一步步地改变。</p>
<p>「规划」即优化每一个子问题。因为MDP 的 Markov 特性，即某一时刻的子问题仅仅取决于上一时刻的子问题的 action，并且 Bellman 方程可以递归地切分子问题。</p>
<h3 id="1-策略估计-Policy-Evaluation"><a href="#1-策略估计-Policy-Evaluation" class="headerlink" title="1. 策略估计(Policy Evaluation)"></a>1. 策略估计(Policy Evaluation)</h3><p>对于任意的策略π，我们如何计算其状态值函数Vπ(s)？这个问题被称作策略估计。</p>
<p>● 基于当前的 Policy 计算出每个状态的 Value function</p>
<p>● 同步更新：每次迭代更新所有的状态的 v<br>\begin{align}<br>V_{k+1}(s)=\sum_{a\in A}\pi(a|s)\left( R_s^a+\gamma\sum_{s’\in S}P_{ss’}^av_k(s’)\right)<br>\end{align}</p>
<p>● 矩阵形式：<br>\begin{align}<br>\mathbf{v^{k+1}=R^{\pi}+\gamma P^{\pi}v^k}<br>\end{align}</p>
<hr>
<center><img src="/image/RL/3/1.jpg" alt="RL"></center><br><center><img src="/image/RL/3/2.jpg" alt="RL"></center> 

<hr>
<p>● 左边是第 k 次迭代每个 state 上状态价值函数的值，右边是通过贪心（greedy）算法找到策略</p>
<p>● 计算实例：<br>\begin{align}<br>k=2, -1.7  \approx -1.75 = 0.25<em>(-1+0) + 0.25</em>(-1-1) + 0.25<em>(-1-1) + 0.25</em>(-1-1)<br>\end{align}<br>\begin{align}<br>k=3, -2.9  \approx -2.925 = -0.25<em>(-1-2) + 0.25</em>(-1-2) + 0.25<em>(-1-2) + 0.25</em>(-1-1.7)<br>\end{align}</p>
<h3 id="2-策略改进-Policy-Improvement"><a href="#2-策略改进-Policy-Improvement" class="headerlink" title="2. 策略改进(Policy Improvement)"></a>2. 策略改进(Policy Improvement)</h3><p>● 基于当前的状态价值函数（value function），用贪心算法找到最优策略</p>
<p>$\pi’(s)=\arg\max_{a\in A} q_{\pi}(s,a)$</p>
<p>● Vπ会一直迭代到收敛，具体证明如图:</p>
<hr>
<center><img src="/image/RL/3/3.jpg" alt="RL"></center> 

<hr>
<p>事实上在大多数情况下 Policy evaluation 不必要非常逼近最优值，这时我们通常引入 \epsilon-convergence 函数来控制迭代停止。</p>
<p>很多情况下价值函数还未完全收敛，Policy 就已经最优，所以在每次迭代之后都可以更新策略（Policy），当策略无变化时停止迭代。</p>
<h3 id="3-策略迭代-Policy-Iteration"><a href="#3-策略迭代-Policy-Iteration" class="headerlink" title="3. 策略迭代(Policy Iteration)"></a>3. 策略迭代(Policy Iteration)</h3><p>策略迭代算法就是策略估计和策略改进两节内容的组合。假设有一个策略π，那么可以用policy evaluation获得它的值函数Vπ(s)，然后根据policy improvement得到更好的策略π’，接着再计算Vπ’(s),再获得更好的策略π’’。</p>
<h3 id="4-价值迭代-Value-Iteration"><a href="#4-价值迭代-Value-Iteration" class="headerlink" title="4. 价值迭代(Value Iteration)"></a>4. 价值迭代(Value Iteration)</h3><p>最优化原理：当且仅当状态 s 达到任意能到达的状态 s‘ 时，价值函数 v 能在当前策略（policy）下达到最优，即$v_{\pi}(s’) = v_<em>(s’)$，与此同时，状态 s 也能基于当前策略达到最优，即$v_{\pi}(s) = v_</em>(s)$</p>
<p>状态转移公式为：<br>$v_{k+1}(s) = \max_{a\in A}(R^a_s+\gamma\sum_{s’ \in S}P^a_{ss’}v_k(s’))$</p>
<p>矩阵形式为：$\mathbf{v_{k+1}} =\max_{a \in A} \mathbf{R^a_s} +\gamma\mathbf{P^av_k})$</p>
<p>下面是一个实例，求每个格子到终点的最短距离，走一步的 reward 是 -1:</p>
<hr>
<center><img src="/image/RL/3/4.png" alt="RL"></center> 

<hr>
<h4 id="同步动态规划算法小结"><a href="#同步动态规划算法小结" class="headerlink" title="同步动态规划算法小结"></a>同步动态规划算法小结</h4><p>MDP 的问题主要分两类：</p>
<p>① Prediction 问题</p>
<ul>
<li><p>输入：MDP (S,A,P,R,y)和策略（policy）π</p>
</li>
<li><p>输出：状态价值函数 Vπ</p>
</li>
</ul>
<p>② Control 问题</p>
<ul>
<li><p>输入：MDP (S,A,P,R.y)</p>
</li>
<li><p>输出：最优状态价值函数V*和最优策略π</p>
</li>
</ul>
<p>● 策略估计(Policy Evaluation)解决的是 Prediction 问题，使用了贝尔曼期望方程（Bellman Expectation Equation）</p>
<p>● 策略改进(Policy Improvement)解决的是 Control 问题，实质是在迭代策略评估之后加一个选择 Policy 的过程，使用的是贝尔曼期望方程和贪心算法</p>
<p>● 策略迭代（Policy Iteration）解决的是 Control 问题，实质是在迭代策略评估之后加一个选择 Policy 的过程，使用的是贝尔曼期望方程和贪心算法</p>
<p>● 价值迭代（Value Iteration） 解决的是 Control 问题，它并没有直接计算策略（Policy），而是在得到最优的基于策略的价值函数之后推导出最优的 Policy，使用的是贝尔曼最优化方程（Bellman Optimality Equation）</p>
<h3 id="参考资料（Reference）"><a href="#参考资料（Reference）" class="headerlink" title="参考资料（Reference）"></a>参考资料（Reference）</h3><ul>
<li><p><a href="https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html" title="Title" target="_blank" rel="external">Dissecting Reinforcement Learning-Part.1</a> </p>
</li>
<li><p><a href="https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/" title="Title" target="_blank" rel="external">REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION</a> </p>
</li>
<li><p><a href="https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" title="Title" target="_blank" rel="external">Pythonではじめる強化学習</a> </p>
</li>
<li><p><a href="https://qiita.com/icoxfog417/items/242439ecd1a477ece312" title="Title" target="_blank" rel="external">ゼロからDeepまで学ぶ強化学習</a> </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral" title="Title" target="_blank" rel="external">强化学习知识整理</a> </p>
</li>
<li><p><a href="http://www.cnblogs.com/jinxulin/p/3526542.html" title="Title" target="_blank" rel="external">动态规划法(Dynamic Programming Methods)</a> </p>
</li>
</ul>

  </section>

  
  
</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; 2014-2015. | 由<a href="https://hexo.io/">Hexo</a>强力驱动 | 主题<a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    

    <script src="/js/jquery.githubRepoWidget.min.js"></script>


    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
