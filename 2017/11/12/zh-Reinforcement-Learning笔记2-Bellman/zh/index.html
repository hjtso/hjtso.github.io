<!DOCTYPE html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Reinforcement Learning笔记2-Bellman | H.J.T. Github 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="H.J.T.">
    
    

    <meta name="description" content="1. Bellman方程（Bellman Equation）贝尔曼方程（Bellman Equation）也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。
贝尔曼方程表明当前状态的值函数与下个状态的值函数的">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning笔记2-Bellman | H.J.T. Github">
<meta property="og:url" content="https://hjtso.github.io/2017/11/12/zh-Reinforcement-Learning笔记2-Bellman/zh/index.html">
<meta property="og:site_name" content="H.J.T. Github">
<meta property="og:description" content="1. Bellman方程（Bellman Equation）贝尔曼方程（Bellman Equation）也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。
贝尔曼方程表明当前状态的值函数与下个状态的值函数的">
<meta property="og:image" content="https://hjtso.github.io/image/RL/2/1.jpg">
<meta property="og:image" content="https://hjtso.github.io/image/RL/2/2.jpg">
<meta property="og:image" content="https://hjtso.github.io/image/RL/2/3.jpg">
<meta property="og:updated_time" content="2022-08-08T09:02:49.619Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning笔记2-Bellman | H.J.T. Github">
<meta name="twitter:description" content="1. Bellman方程（Bellman Equation）贝尔曼方程（Bellman Equation）也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。
贝尔曼方程表明当前状态的值函数与下个状态的值函数的">
<meta name="twitter:image" content="https://hjtso.github.io/image/RL/2/1.jpg">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon-black-55.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">H.J.T. Github</a></h1>
        <hr class="panel-cover__divider" />

        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">See</a></li>
              
                
                <li class="navigation__item"><a href="/About" title="" class="">Key</a></li>
              
                
                <li class="navigation__item"><a href="/Archive" title="" class="">Arc</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">

    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/hjtso" title="Huno on GitHub">
          <i class='icon icon-social-github'></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

      <!-- Facebook -->
      <li class="navigation__item">
        <a href="https://www.facebook.com/jintao.huang.1" title=“Facebook”>
          <i class='icon icon-social-facebook'></i>
          <span class="label">Facebook</span>
        </a>
      </li>

      <!-- Instagram -->
      <li class="navigation__item">
        <a href="https://www.instagram.com/huang_jintao/" title=“Instagram”>
          <i class='icon icon-social-instagram'></i>
          <span class="label">Instagram</span>
        </a>
      </li>

      <!-- Linkedin -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/jintao-huang-48429565" title=“Linkedin”>
          <i class='icon icon-social-linkedin'></i>
          <span class="label">Linkedin</span>
        </a>
      </li>
	  
    <!-- China social icon -->

      <li class="navigation__item">
        <a href="http://www.weibo.com/musimusi" title=“Weibo”>
          <i class='icon cs-icon-weibo'></i>
          <span class="label">Weibo</span>
        </a>
      </li>

    <!--
      <li class="navigation__item">
        <a href="https://www.hjt.so" title=“_hjtso”>
          <i class='icon cs-icon-wechat'></i>
          <span class="label">Wechat</span>
        </a>
      </li>


      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li>
    -->


  </ul>
</nav>



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Reinforcement Learning笔记2-Bellman</h1>

    

    <div class="post-meta">
      <time datetime="2017-11-12" class="post-meta__date date">2017-11-12</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <h3 id="1-Bellman方程（Bellman-Equation）"><a href="#1-Bellman方程（Bellman-Equation）" class="headerlink" title="1. Bellman方程（Bellman Equation）"></a>1. Bellman方程（Bellman Equation）</h3><p>贝尔曼方程（Bellman Equation）也被称作动态规划方程（Dynamic Programming Equation），由理查·贝尔曼（Richard Bellman）发现。贝尔曼方程是动态规划（Dynamic Programming）这些数学最佳化方法能够达到最佳化的必要条件。</p>
<p>贝尔曼方程表明当前状态的值函数与下个状态的值函数的关系。</p>
<p>见下图，第一层的空心圆代表当前状态（state），向下连接的实心圆代表当前状态可以执行两个动作，第三层代表执行完某个动作后可能到达的状态 s’。</p>
<hr>
<center><img src="/image/RL/2/1.jpg" alt="RL"></center> 

<hr>
<p>根据上图得出状态价值函数公式：</p>
<p>\begin{align}<br>v_{\pi}(s)=E[U_t|S_t=s]=\sum_{a\in A}\pi(a|s)\,\left( R_s^a+\gamma \sum_{s’\in S}P^a_{ss’}v_{\pi}(s’)\right)<br>\end{align}</p>
<p>其中：</p>
<p>\begin{align}<br>P_{ss’}^a = P(S_{t+1}=s’|S_t=s, A_t=a)<br>\end{align}</p>
<p>上式中策略π是指给定状态 s 的情况下，动作 a 的概率分布，即：</p>
<p>\begin{align}<br>\pi(a|s)=P(a|s)<br>\end{align}</p>
<p>将概率和转换为期望，上式等价于：</p>
<p>\begin{align}<br>v_{\pi}(s) = E_{\pi}[R_s^a + \gamma v_{\pi}(S_{t+1}|S_t=s]<br>\end{align}</p>
<p>同理，我们可以得到动作价值函数的公式如下：</p>
<p>\begin{align}<br>q_{\pi}(s,a)=E_{\pi}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_t=s,A_t=a]<br>\end{align}</p>
<hr>
<center><img src="/image/RL/2/2.jpg" alt="RL"></center> 

<hr>
<p>如上图，Bellman 方程也可以表达成矩阵形式：</p>
<p>\begin{align}<br>v=R+\gamma Pv<br>\end{align}</p>
<p>可直接求出：</p>
<p>\begin{align}<br>v=(I-\gamma P)^{-1}R<br>\end{align}</p>
<p>其复杂度为：</p>
<p>\begin{align}<br>O(n^3)<br>\end{align}</p>
<p>一般可通过动态规划、时间差分法与蒙特卡洛估计求解。</p>
<p>● 通过解 Bellman 最优性方程找一个最优策略需要以下条件:</p>
<ul>
<li><p>动态模型已知</p>
</li>
<li><p>拥有足够的计算空间和时间</p>
</li>
<li><p>系统满足 Markov 特性</p>
</li>
</ul>
<h3 id="2-状态价值函数和动作价值函数的关系"><a href="#2-状态价值函数和动作价值函数的关系" class="headerlink" title="2. 状态价值函数和动作价值函数的关系"></a>2. 状态价值函数和动作价值函数的关系</h3><hr>
<center><img src="/image/RL/2/3.jpg" alt="RL"></center> 

<hr>
<p>\begin{align}<br>v_{\pi}(s) = \sum_{a \in A} \pi(a|s)q_{\pi}(s,a) = E[q_{\pi}(s,a)|S_t=s]<br>\end{align}</p>
<p>\begin{align}<br>q_{\pi}(s,a) = R_s^a + \gamma \sum_{s’\in S}P_{ss’}^a \sum_{a’ \in A}\pi(a’|s’)q_{\pi}(s’,a’) = R_s^a+\gamma \sum_{s’ \in S} P_{ss’}^a v_{\pi}(s’)<br>\end{align}</p>
<h3 id="3-最优方程"><a href="#3-最优方程" class="headerlink" title="3. 最优方程"></a>3. 最优方程</h3><p>\begin{align}<br>v_<em>(s)= \max_aq_</em>(s,a)= \max_a\left( R_s^a + \gamma\sum_{s’\in S}P_{ss’}^a v_<em>(s’) \right) \\<br>q_</em>(s,a)= R_s^a+\gamma \sum_{s’\in S}P_{ss’}^av_<em>(s’) = R_s^a + \gamma \sum_{s’ \in S}P_{ss’}^a\max_{a’}q_</em>(s’,a’)<br>\end{align}</p>
<p>● v 描述了处于一个状态的长期最优化价值，即在这个状态下考虑到所有可能发生的后续动作，并且都挑选最优的动作来执行的情况下，这个状态的价值</p>
<p>● q 描述了处于一个状态并执行某个动作后所带来的长期最优价值，即在这个状态下执行某一特定动作后，考虑再之后所有可能处于的状态并且在这些状态下总是选取最优动作来执行所带来的长期价值</p>
<h3 id="4-最优策略"><a href="#4-最优策略" class="headerlink" title="4. 最优策略"></a>4. 最优策略</h3><p>关于收敛性：（对策略定义一个偏序）</p>
<p>$\pi \ge \pi’ \,\mbox{if}\; v_{\pi}(s)\ge v_{\pi’}(s),\forall s$</p>
<p>定理：</p>
<p>对于任意 MDP：</p>
<ul>
<li><p>总是存在一个最优策略π*，它比其它任何策略都要好，或者至少一样好</p>
</li>
<li><p>所有最优决策都达到最优值函数， $v_{\pi_<em>}(s)=v_</em>(s)$</p>
</li>
<li><p>所有最优决策都达到最优行动值函数，$q_{\pi_<em>}(s,a)=q_</em>(s,a)$</p>
</li>
</ul>
<p>最优策略可从最优状态价值函数或者最优动作价值函数得出：</p>
<p>\begin{align}<br>\pi_<em>(a|s) =<br>\begin{cases}<br>1, &amp; \mbox{if } a = \arg\max_{a \in A} q_</em>(s,a)\\<br>0, &amp; otherwise<br>\end{cases}<br>\end{align}</p>
<h3 id="参考资料（Reference）"><a href="#参考资料（Reference）" class="headerlink" title="参考资料（Reference）"></a>参考资料（Reference）</h3><ul>
<li><p><a href="https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html" title="Title" target="_blank" rel="external">Dissecting Reinforcement Learning-Part.1</a> </p>
</li>
<li><p><a href="https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/" title="Title" target="_blank" rel="external">REINFORCEMENT LEARNING PART 1: Q-LEARNING AND EXPLORATION</a> </p>
</li>
<li><p><a href="https://qiita.com/Hironsan/items/56f6c0b2f4cfd28dd906" title="Title" target="_blank" rel="external">Pythonではじめる強化学習</a> </p>
</li>
<li><p><a href="https://qiita.com/icoxfog417/items/242439ecd1a477ece312" title="Title" target="_blank" rel="external">ゼロからDeepまで学ぶ強化学習</a> </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral" title="Title" target="_blank" rel="external">强化学习知识整理</a> </p>
</li>
</ul>

  </section>

  
  
</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; H.J.T. | <a href="https://hexo.io/">Hexo</a> | Theme: <a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    

    <script src="/js/jquery.githubRepoWidget.min.js"></script>


    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
