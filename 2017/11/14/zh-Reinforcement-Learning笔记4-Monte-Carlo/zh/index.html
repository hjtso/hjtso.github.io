<!DOCTYPE html>
<html>
<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    

    <title>
      Reinforcement Learning笔记4-Monte Carlo | H.J.T. Github 
    </title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    
      <meta name="author" content="H.J.T.">
    
    

    <meta name="description" content="Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）蒙特卡罗方法又叫统计模拟方法，它使用随机数（或伪随机数）来解决计算的问题，是一类重要的数值计算方法。该方法的名字来源于世界著名的赌城蒙特卡罗，而蒙特卡罗方法正是以概率为基础的方法。
一个简单的例子可以解释蒙特卡罗方法，假设我们需要计算一个不规则图形的面积，那么图形的不规则程度和分析性计算（比如积分）的复杂程度是成正">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning笔记4-Monte Carlo | H.J.T. Github">
<meta property="og:url" content="https://hjtso.github.io/2017/11/14/zh-Reinforcement-Learning笔记4-Monte-Carlo/zh/index.html">
<meta property="og:site_name" content="H.J.T. Github">
<meta property="og:description" content="Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）蒙特卡罗方法又叫统计模拟方法，它使用随机数（或伪随机数）来解决计算的问题，是一类重要的数值计算方法。该方法的名字来源于世界著名的赌城蒙特卡罗，而蒙特卡罗方法正是以概率为基础的方法。
一个简单的例子可以解释蒙特卡罗方法，假设我们需要计算一个不规则图形的面积，那么图形的不规则程度和分析性计算（比如积分）的复杂程度是成正">
<meta property="og:image" content="https://hjtso.github.io/image/RL/4/0-1.png">
<meta property="og:image" content="https://hjtso.github.io/image/RL/4/2-1.svg">
<meta property="og:image" content="https://hjtso.github.io/image/RL/4/2-2.png">
<meta property="og:image" content="https://hjtso.github.io/image/RL/4/2-3.svg">
<meta property="og:updated_time" content="2022-08-08T09:02:53.787Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning笔记4-Monte Carlo | H.J.T. Github">
<meta name="twitter:description" content="Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）蒙特卡罗方法又叫统计模拟方法，它使用随机数（或伪随机数）来解决计算的问题，是一类重要的数值计算方法。该方法的名字来源于世界著名的赌城蒙特卡罗，而蒙特卡罗方法正是以概率为基础的方法。
一个简单的例子可以解释蒙特卡罗方法，假设我们需要计算一个不规则图形的面积，那么图形的不规则程度和分析性计算（比如积分）的复杂程度是成正">
<meta name="twitter:image" content="https://hjtso.github.io/image/RL/4/0-1.png">
    
    
    
      <link rel="icon" type="image/x-icon" href="/favicon-black-55.png">
    
    <link rel="stylesheet" href="/css/uno.css">
    <link rel="stylesheet" href="/css/highlight.css">
    <link rel="stylesheet" href="/css/archive.css">
    <link rel="stylesheet" href="/css/china-social-icon.css"><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>
<body>

    <!-- hexo-inject:begin --><!-- hexo-inject:end --><span class="mobile btn-mobile-menu">
        <i class="icon icon-list btn-mobile-menu__icon"></i>
        <i class="icon icon-x-circle btn-mobile-close__icon hidden"></i>
    </span>

    

<header class="panel-cover panel-cover--collapsed">


  <div class="panel-main">

  
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        

        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage">H.J.T. Github</a></h1>
        <hr class="panel-cover__divider" />

        

        <div class="navigation-wrapper">

          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">

              
                
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">See</a></li>
              
                
                <li class="navigation__item"><a href="/About" title="" class="">Key</a></li>
              
                
                <li class="navigation__item"><a href="/Archive" title="" class="">Arc</a></li>
              

            </ul>
          </nav>

          <!-- ----------------------------
To add a new social icon simply duplicate one of the list items from below
and change the class in the <i> tag to match the desired social network
and then add your link to the <a>. Here is a full list of social network
classes that you can use:

    icon-social-500px
    icon-social-behance
    icon-social-delicious
    icon-social-designer-news
    icon-social-deviant-art
    icon-social-digg
    icon-social-dribbble
    icon-social-facebook
    icon-social-flickr
    icon-social-forrst
    icon-social-foursquare
    icon-social-github
    icon-social-google-plus
    icon-social-hi5
    icon-social-instagram
    icon-social-lastfm
    icon-social-linkedin
    icon-social-medium
    icon-social-myspace
    icon-social-path
    icon-social-pinterest
    icon-social-rdio
    icon-social-reddit
    icon-social-skype
    icon-social-spotify
    icon-social-stack-overflow
    icon-social-steam
    icon-social-stumbleupon
    icon-social-treehouse
    icon-social-tumblr
    icon-social-twitter
    icon-social-vimeo
    icon-social-xbox
    icon-social-yelp
    icon-social-youtube
    icon-social-zerply
    icon-mail

-------------------------------->

<!-- add social info here -->



<nav class="cover-navigation navigation--social">
  <ul class="navigation">

    
      <!-- Github -->
      <li class="navigation__item">
        <a href="https://github.com/hjtso" title="Huno on GitHub">
          <i class='icon icon-social-github'></i>
          <span class="label">GitHub</span>
        </a>
      </li>
    

      <!-- Facebook -->
      <li class="navigation__item">
        <a href="https://www.facebook.com/jintao.huang.1" title=“Facebook”>
          <i class='icon icon-social-facebook'></i>
          <span class="label">Facebook</span>
        </a>
      </li>

      <!-- Instagram -->
      <li class="navigation__item">
        <a href="https://www.instagram.com/huang_jintao/" title=“Instagram”>
          <i class='icon icon-social-instagram'></i>
          <span class="label">Instagram</span>
        </a>
      </li>

      <!-- Linkedin -->
      <li class="navigation__item">
        <a href="https://www.linkedin.com/in/jintao-huang-48429565" title=“Linkedin”>
          <i class='icon icon-social-linkedin'></i>
          <span class="label">Linkedin</span>
        </a>
      </li>
	  
    <!-- China social icon -->

      <li class="navigation__item">
        <a href="http://www.weibo.com/musimusi" title=“Weibo”>
          <i class='icon cs-icon-weibo'></i>
          <span class="label">Weibo</span>
        </a>
      </li>

    <!--
      <li class="navigation__item">
        <a href="https://hjtso.github.io" title=“_hjtso”>
          <i class='icon cs-icon-wechat'></i>
          <span class="label">Wechat</span>
        </a>
      </li>


      <li class="navigation__item">
        <a href="" title="">
          <i class='icon cs-icon-douban'></i>
          <span class="label">Douban</span>
        </a>
      </li>
    -->


  </ul>
</nav>



        </div>

      </div>

    </div>

    <div class="panel-cover--overlay"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner entry">
            

<article class="post-container post-container--single">

  <header class="post-header">
    
    <h1 class="post-title">Reinforcement Learning笔记4-Monte Carlo</h1>

    

    <div class="post-meta">
      <time datetime="2017-11-14" class="post-meta__date date">2017-11-14</time> 

      <span class="post-meta__tags tags">

          
            <font class="categories">
            &#8226; 分类:
            <a class="categories-link" href="/categories/Reinforcement-Learning/">Reinforcement Learning</a>
            </font>
          

          
             &#8226; 标签:
            <font class="tags">
              <a class="tags-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a>
            </font>
          

      </span>
    </div>
    
    

  </header>

  <section id="post-content" class="article-content post">
    <h4 id="Bellman最优解策略-蒙特卡罗方法（Monte-Carlo-Methods）"><a href="#Bellman最优解策略-蒙特卡罗方法（Monte-Carlo-Methods）" class="headerlink" title="Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）"></a>Bellman最优解策略 - 蒙特卡罗方法（Monte Carlo Methods）</h4><p>蒙特卡罗方法又叫统计模拟方法，它使用随机数（或伪随机数）来解决计算的问题，是一类重要的数值计算方法。该方法的名字来源于世界著名的赌城蒙特卡罗，而蒙特卡罗方法正是以概率为基础的方法。</p>
<p>一个简单的例子可以解释蒙特卡罗方法，假设我们需要计算一个不规则图形的面积，那么图形的不规则程度和分析性计算（比如积分）的复杂程度是成正比的。而采用蒙特卡罗方法是怎么计算的呢？首先你把图形放到一个已知面积的方框内，然后假想你有一些豆子，把豆子均匀地朝这个方框内撒，散好后数这个图形之中有多少颗豆子，再根据图形内外豆子的比例来计算面积。当你的豆子越小，撒的越多的时候，结果就越精确。</p>
<p>强化学习方法分类：</p>
<hr>
<center><img src="/image/RL/4/0-1.png" alt="RL"></center> 

<hr>
<p>若已知模型时，马尔科夫决策过程可以利用动态规划的方法来解决。</p>
<p>无模型的强化学习算法主要包括蒙特卡罗方法和时间差分方法。</p>
<h3 id="1-蒙特卡罗方法（Monte-Carlo）"><a href="#1-蒙特卡罗方法（Monte-Carlo）" class="headerlink" title="1. 蒙特卡罗方法（Monte Carlo）"></a>1. 蒙特卡罗方法（Monte Carlo）</h3><p>蒙特卡罗方法仅仅需要经验就可以求解最优策略，这些经验可以在线获得或者根据某种模拟机制获得。</p>
<p>要注意的是，仅将蒙特卡罗方法定义在episode task上，所谓的episode task就是指不管采取哪种策略π，都会在有限时间内到达终止状态并获得回报的任务。比如玩棋类游戏，在有限步数以后总能达到输赢或者平局的结果并获得相应回报。</p>
<p>那么什么是经验呢？经验其实就是训练样本。比如在初始状态s，遵循策略π，最终获得了总回报R，这就是一个样本。如果我们有许多这样的样本，就可以估计在状态s下，遵循策略π的期望回报，也就是状态值函数Vπ(s)了。蒙特卡罗方法就是依靠样本的平均回报来解决增强学习问题的。</p>
<h3 id="2-蒙特卡罗策略改进（Monte-Carlo-Policy-Evalution）"><a href="#2-蒙特卡罗策略改进（Monte-Carlo-Policy-Evalution）" class="headerlink" title="2. 蒙特卡罗策略改进（Monte Carlo Policy Evalution）"></a>2. 蒙特卡罗策略改进（Monte Carlo Policy Evalution）</h3><p>内容来自：<a href="https://zhuanlan.zhihu.com/p/25743759" title="Title" target="_blank" rel="external">强化学习入门 第三讲 蒙特卡罗方法</a> </p>
<p><strong>探索的必要性：</strong>状态值函数和行为值函数的计算实际上是计算返回值的期望。动态规划的方法是利用模型对该期望进行计算。在没有模型时，我们可以采用蒙特卡罗的方法计算该期望，即利用随机样本来估计期望。在计算值函数时，蒙特卡罗方法是利用 <strong>经验平均</strong> 代替随机变量的期望。</p>
<p>如何获得充足的经验是无模型强化学习的核心所在。</p>
<p>在动态规划方法中，为了保证值函数的收敛性，算法会对状态空间中的状态进行逐个扫描。无模型的方法充分评估策略值函数的前提是每个状态都能被访问到。因此，在蒙特卡洛方法中必须采用一定的方法保证每个状态都能被访问到。其中一种方法是探索性初始化。</p>
<p><strong>探索性初始化：</strong>是指每个状态都有一定的几率作为初始状态。在给出基于探索性初始化的蒙特卡罗方法前，我们还需要给出策略改进方法，以及便于进行迭代计算的平均方法。</p>
<p><strong>蒙特卡罗策略改进：</strong>蒙特卡罗方法利用经验平均对策略值函数进行估计。当值函数被估计出来后，对于每个状态s ，通过最大化动作值函数，来进行策略的改进。即：</p>
<p>\begin{align}<br>\pi\left(s\right)=\underset{a}{arg\max\textrm{ }}q\left(s,a\right)<br>\end{align}</p>
<p>递增计算平均的方法：</p>
<hr>
<center><img src="/image/RL/4/2-1.svg" alt="RL"></center> 

<hr>
<hr>
<center><img src="/image/RL/4/2-2.png" alt="RL"></center> 

<hr>
<p>在探索性初始化中，迭代每一幕时，初始状态是随机分配的，这样可以保证迭代过程中每个状态行为对都能被选中。它蕴含着一个假设，即：假设所有的动作都被无限频繁选中。对于这个假设，有时很难成立，或无法完全保证。</p>
<p>● 如何保证初始状态不变的同时，又能保证报个状态行为对可以被访问到？</p>
<p>答案是：精心地设计你的探索策略，以保证每个状态都能被访问到。</p>
<p>● 可是如何精心地设计探索策略？符合要求的探索策略是什么样的？</p>
<p>答案是：策略必须是温和的，即对所有的状态s 和a 满足：</p>
<p>\begin{align}<br>\pi\left(a|s\right)&gt;0<br>\end{align}</p>
<p>也就是说，温和的探索策略是指在任意状态下，采用动作集中每个动作的概率都大于零。典型的温和策略是</p>
<p>\begin{align}<br>\varepsilon -soft<br>\end{align}</p>
<p>即：</p>
<hr>
<center><img src="/image/RL/4/2-3.svg" alt="RL"></center> 

<hr>
<p>根据探索策略（行动策略）和评估的策略是否是同一个策略，蒙特卡罗方法又分为on-policy和off-policy.</p>
<p>● 若行动策略和评估及改善的策略是同一个策略，称之为on-policy,可翻译为同策略。</p>
<p>● 若行动策略和评估及改善的策略是不同的策略，称之为off-policy,可翻译为异策略。</p>
<p><strong>まとめ：</strong> 以上大概讲解了如何利用MC的方法估计值函数。跟基于动态规划的方法相比，基于MC的方法只是在值函数估计上有所不同。两者在整个框架上是相同的，即对当前策略进行评估，然后利用学到的值函数进行策略改进。</p>
<h3 id="3-蒙特卡罗（Monte-Carlo）VS-拉斯维加斯（Las-Vegas）"><a href="#3-蒙特卡罗（Monte-Carlo）VS-拉斯维加斯（Las-Vegas）" class="headerlink" title="3. 蒙特卡罗（Monte Carlo）VS 拉斯维加斯（Las Vegas）"></a>3. 蒙特卡罗（Monte Carlo）VS 拉斯维加斯（Las Vegas）</h3><p>内容来自：<a href="https://www.zhihu.com/question/20254139/answer/33572009" title="Title" target="_blank" rel="external">知乎：蒙特卡罗算法是什么？- 孙天齐回答</a> </p>
<p>这里把随机算法分成两类：</p>
<ul>
<li><p>蒙特卡罗算法：采样越多，越近似最优解；</p>
</li>
<li><p>拉斯维加斯算法：采样越多，越有机会找到最优解；</p>
</li>
</ul>
<p>● 举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——尽量找好的，但不保证是最好的。</p>
<p>● 而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（最优解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——尽量找最好的，但不保证能找到。</p>
<p>这两个词并不深奥，它只是概括了随机算法的特性，算法本身可能复杂，也可能简单。这两个词本身是两座著名赌城，因为赌博中体现了许多随机算法，所以借过来命名。这两类随机算法之间的选择，往往受到问题的局限。如果问题要求在有限采样内，必须给出一个解，但不要求是最优解，那就要用蒙特卡罗算法。反之，如果问题要求必须给出最优解，但对采样没有限制，那就要用拉斯维加斯算法。</p>
<p>对于机器围棋程序而言，因为每一步棋的运算时间、堆栈空间都是有限的，而且不要求最优解，所以ZEN涉及的随机算法，肯定是蒙特卡罗式的。机器下棋的算法本质都是搜索树，围棋难在它的树宽可以达到好几百（国际象棋只有几十）。在有限时间内要遍历这么宽的树，就只能牺牲深度（俗称“往后看几步”），但围棋又是依赖远见的游戏，甚至不仅是看“几步”的问题。所以，要想保证搜索深度，就只能放弃遍历，改为随机采样——这就是为什么在没有MCTS（蒙特卡罗搜树）类的方法之前，机器围棋的水平几乎是笑话。而采用了MCTS方法后，搜索深度就大大增加了。</p>
<h3 id="参考资料（Reference）"><a href="#参考资料（Reference）" class="headerlink" title="参考资料（Reference）"></a>参考资料（Reference）</h3><ul>
<li><p><a href="https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html" title="Title" target="_blank" rel="external">Dissecting Reinforcement Learning-Part.1</a> </p>
</li>
<li><p><a href="https://qiita.com/icoxfog417/items/242439ecd1a477ece312" title="Title" target="_blank" rel="external">ゼロからDeepまで学ぶ強化学習</a> </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/25743759" title="Title" target="_blank" rel="external">强化学习入门 第三讲 蒙特卡罗方法</a> </p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/25319023?utm_source=tuicool&amp;utm_medium=referral" title="Title" target="_blank" rel="external">强化学习知识整理</a> </p>
</li>
<li><p><a href="http://www.cnblogs.com/jinxulin/p/3560737.html" title="Title" target="_blank" rel="external">蒙特卡罗方法(Monte Carlo Methods)</a> </p>
</li>
</ul>

  </section>

  
  
</article>


            <footer class="footer">

    <span class="footer__copyright">&copy; H.J.T. | <a href="https://hexo.io/">Hexo</a> | Theme: <a href="https://github.com/someus/huno">Huno</a></span>
    
</footer>
        </div>
    </div>

    <!-- js files -->
    <script src="/js/jquery.min.js"></script>
    <script src="/js/main.js"></script>
    <script src="/js/scale.fix.js"></script>
    

    

    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript"> 
        $(document).ready(function(){
            MathJax.Hub.Config({ 
                tex2jax: {inlineMath: [['[latex]','[/latex]'], ['\\(','\\)']]} 
            });
        });
    </script>


    

    <script src="/js/awesome-toc.min.js"></script>
    <script>
        $(document).ready(function(){
            $.awesome_toc({
                overlay: true,
                contentId: "post-content",
            });
        });
    </script>


    
    

    <script src="/js/jquery.githubRepoWidget.min.js"></script>


    <!--kill ie6 -->
<!--[if IE 6]>
  <script src="//letskillie6.googlecode.com/svn/trunk/2/zh_CN.js"></script>
<![endif]--><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>
