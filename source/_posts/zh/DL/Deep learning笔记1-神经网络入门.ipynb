{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Deep learning笔记1：神经网络入门\n",
    "lang: zh\n",
    "date: 2017-08-08 18:18:56\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 简单的神经网络（Neural Network）\n",
    "\n",
    "![simple-neuron](/image/simple-neuron.png)\n",
    "\n",
    "- 神经网络示意图，圆圈代表单元，方块是运算\n",
    "\n",
    "- 这个架构使得神经网络可以实现，激活函数 f(h) 可以是任何函数，此例用\n",
    "\n",
    "$$sigmoid(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    #TODO: Implement sigmoid function\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "#Calculate the output\n",
    "output = sigmoid(np.dot(weights, inputs) + bias)\n",
    "\n",
    "print('Output:')\n",
    "print(output)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 重要的三个函数（Function）\n",
    "\n",
    "- h(x) 即模型，也就是从输入特征预测输入的那个函数\n",
    "\n",
    "$$h(x) = w_{1}x_{1} + w_{2}x_{2} + … + b , \\text{其中w为权值，b为偏置项}$$\n",
    "\n",
    "- E(w) 目标损失函数 目标函数取最小(最大)值时所对应的参数值，就是模型的参数的最优值。很多时候我们只能获得目标函数的局部最小(最大)值，因此也只能得到模型参数的局部最优值。eg:\n",
    "\n",
    " ① 最小二乘式（一般用于回归类问题）：\n",
    "$$E(w) = \\frac{1}{2}\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}})^{2}，其中，梯度 \\Delta E(w) = -\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}}) x^{i}$$\n",
    "\n",
    " ② 交叉熵式（一般用于分类问题）：\n",
    "$$L(y,o)=-\\frac{1}{N}\\sum_{n\\in{N}}{y_nlogo_n}$$\n",
    "\n",
    "- f(x) 激活函数，常用的有sigmoid／relu／softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 随机梯度下降（Stochastic Gradient Descent,）\n",
    "\n",
    "- 将预测值与实际值的误差$|y^{0}-y^{t}|$，组成的一个抛物线函数$|{y^{0}-y^{t}}|^2$，在抛物线里一直找梯度向下的方向，乘以步长$\\eta$（也称作学习信率），不断地迭代，找出最小值。\n",
    "\n",
    "\n",
    "- 梯度下降算法：\n",
    "$$\\mathrm{x}_{new}=\\mathrm{x}_{old}-\\eta\\nabla{f(x)}\\qquad(式3.1)$$\n",
    "其中△为梯度算子，△f(x)是指f(x)的梯度，$\\eta$步长\n",
    "\n",
    "\n",
    "- 梯度下降算法可以写成：(△E(w)详细推导请看 [△E(w)的推导](https://www.zybuluo.com/hanbingtao/note/448086/ \"Title\") )\n",
    "\n",
    "$$\\mathrm{w}_{new}=\\mathrm{w}_{old}-\\eta\\nabla{E(\\mathrm{w})}\\qquad(式3.2)$$\n",
    "\n",
    "$$由于梯度：  △E(w) = -\\sum\\limits_{i=1}^n(y^{i}-\\overline{y^{i}}) x^{i} $$\n",
    "\n",
    "\n",
    "\n",
    "- 因此，线性单元的参数修改规则最后是:\n",
    "\n",
    "$$\\mathrm{w}_{new}=\\mathrm{w}_{old}+\\eta\\sum_{i=1}^{n}(y^{(i)}-\\bar{y}^{(i)})\\mathrm{x}^{(i)}\\qquad(式3.3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4. 前向传播（Forward Propagation）\n",
    "\n",
    "- 在完成网络的每个层级时，计算每个神经元的输出。一个层级的所有输出变成下一层级神经元的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 反向传播（Back Propagation）\n",
    "\n",
    "- 在神经网络中使用权重将信号从输入层传播到输出层。使用权重将错误从输出层传播回网络，以便更新权重。\n",
    "\n",
    "\n",
    "- 按照下面的方法计算出每个节点的误差项（error term） $\\delta_i$\n",
    "\n",
    "\n",
    "- 对于输出层节点$i$，\n",
    "$$\\delta_i=y_i(1-y_i)(t_i-y_i)\\qquad(式5.1)$$\n",
    "\n",
    "\n",
    "- 对于隐藏层节点，\n",
    "$$\\delta_i=a_i(1-a_i)\\sum_{k\\in{outputs}}w_{ki}\\delta_k\\qquad(式5.2)$$\n",
    "\n",
    "\n",
    "- 梯度下降公式可表达为：\n",
    "$$w_{ji}\\gets w_{ji}+\\eta\\delta_jx_{ji}\\qquad(式5.3)$$\n",
    "\n",
    "\n",
    "- 反向传播算法的详细推导请看（[反向传播算法的推导](https://www.zybuluo.com/hanbingtao/note/476663/ \"Title\") ）\n",
    "\n",
    "- 关于反向传播的推荐阅读（[CS224n笔记5 反向传播与项目指导 - Hankcs](http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html \"Title\") ）\n",
    "\n",
    "- [动态图表示前向和后向传播全过程 - daniel-D](http://www.cnblogs.com/daniel-D/archive/2013/06/03/3116278.html \"Title\") \n",
    "<center>![simple-neuron](/image/DL/1/BP.gif)</center>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 超参数的确定（Adjust Parameters）\n",
    "\n",
    "#### 选择迭代次数\n",
    "\n",
    "也就是训练网络时从训练数据中抽样的批次数量。迭代次数越多，模型就与数据越拟合。但是，如果迭代次数太多，模型就无法很好地泛化到其他数据，这叫做过拟合。你需要选择一个使训练损失很低并且验证损失保持中等水平的数字。当你开始过拟合时，你会发现训练损失继续下降，但是验证损失开始上升。\n",
    "\n",
    "#### 选择学习速率$\\eta$\n",
    "\n",
    "速率可以调整权重更新幅度。如果速率太大，权重就会太大，导致网络无法与数据相拟合。建议从 0.1 开始。如果网络在与数据拟合时遇到问题，尝试降低学习速率。注意，学习速率越低，权重更新的步长就越小，神经网络收敛的时间就越长。\n",
    "\n",
    "\n",
    "#### 选择隐藏节点数量\n",
    "\n",
    "隐藏节点越多，模型的预测结果就越准确。尝试不同的隐藏节点的数量，看看对性能有何影响。你可以查看损失字典，寻找网络性能指标。如果隐藏单元的数量太少，那么模型就没有足够的空间进行学习，如果太多，则学习方向就有太多的选择。选择隐藏单元数量的技巧在于找到合适的平衡点。\n",
    "\n",
    "- 调整根据经验，下面有几个经验公式：\n",
    "\n",
    "\\begin{align}\n",
    "&m=\\sqrt{n+l}+\\alpha\\\\\n",
    "&m=log_2n\\\\\n",
    "&m=\\sqrt{nl}\\\\\n",
    "&m:隐藏层节点数\\\\\n",
    "&n:输入层节点数\\\\\n",
    "&l:输出层节点数\\\\\n",
    "&\\alpha:1到10之间的常数\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 程序实例（Program Example）\n",
    "\n",
    "- [Github Link](https://github.com/HJTSO/first-neural-network/blob/master/Your_first_neural_network.ipynb/ \"Title\") \n",
    "\n",
    "### 参考资料（Reference）\n",
    "\n",
    "- [梯度下降](https://www.zybuluo.com/hanbingtao/note/448086/ \"Title\") \n",
    "\n",
    "\n",
    "- [反向传播以及超参数的确定](https://www.zybuluo.com/hanbingtao/note/476663/ \"Title\") \n",
    "  \n",
    "  \n",
    "- [CS224n笔记5 反向传播与项目指导 - Hankcs](http://www.hankcs.com/nlp/cs224n-backpropagation-and-project-advice.html \"Title\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
